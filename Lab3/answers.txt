
#########################################################################################
[LABORATION] Plagiarism detection
##
## Important note: This is a machine-readable config file that will be read using 
## the Python 'configparser' library: https://docs.python.org/3/library/configparser.html
##
## Please follow these rules when editing this file:
##  1. Only write your answer where there is a [...] stub.
##  2. Multiline answers must be indented.
##  3. Don't change anything else.
#########################################################################################

# Which programming language did you implement in? 
# (Write only Java, Python, or PyPy)

Language: [Java]


# Who are you?

Group members: 
    [Jonatan Cederberg]
    [Isac Snecker]
    [Reza Amani]


#########################################################################################
[PART 1] Complexity analysis
#########################################################################################

# What is the asymptotic complexity of 'computeIntersections'? Justify your answer.
# Answer in terms of N, the total number of 5-grams in the input files.
#
# You may assume that there are D documents and they all have the same length,
# i.e., they have the same number, K, of 5-grams. 
# You may also assume that there is not much plagiarised text; 
# specifically, that most 5-grams occur in only one file, and that 
# the number of duplicate occurrences of 5-grams is a small *constant*.

Q1a:
    Complexity:    [N^2]
    Justification: [There are D documents, and each gets compared to all the others through nested looping giving a complexity of D^2,
    for each iteration of the double loop, the 5-grams of each document gets compared through further nested looping,
    if let through the if statement, but this will only affect the total complexity by a factor,
    K 5-grams in each gives this a further complexity of D^2*K^2. If we define N as D*K the complexity is described by O(N^2).]


# How long did the program take compute the intersections for the 'tiny', 'small' and 'medium' directories?
# (You can skip the 'medium' directory if it takes more than 5 minutes).

Q1b:
    tiny:   [1.2s]
    small:  [4.7s]
    medium: [102.2s]


# Is the ratio between the times what you would expect, given the asymptotic complexity?
# Explain very briefly why.

Q1c:
    [It is to be expected, since the number of 5-ngrams doubles between tiny and small the time it take should be four times longer,
     as the time it takes depends on the complexity and the complexity is quadratic. Same goes for the jump between small to medium
     which is five times bigger, meaning an expected 25 times increase in time taken.]


# How long do you predict the program would take to compute the intersections
# for the 'big' and 'huge' directories, respectively? Show your calculations.
# (Do the same for the 'medium' directory too if you didn't run it in Q1b.)

Q1d:
    big:  [10220.0s, since the size-change between medium and big is 10 times larger, meaning the time-difference is 10^2 = 100]
    huge: [163520.0s, since the size change between big and huge is 4 times larger, meaning the time-difference is 4^2 = 16]


#########################################################################################
[PART 2] Using an intermediate data structure
#########################################################################################

# How long time does it take to (1) build the n-gram index, and (2) compute the intersections, 
# for the 'small' directory?

Q2a: 
    buildNgramIndex:      [1.56s]
    computeIntersections: [1.79s]

# Was this an improvement compared to the original implementation? (see Q1b)

Q2b: [Yes somewhat, but not that much since building n-gram indexes now takes some time.]


#########################################################################################
[PART 3] Implementing a BST
#########################################################################################

# How long time does it take to (1) build the n-gram index, and (2) compute the intersections, 
# for the 'tiny', 'small' and 'medium' directories? 
# (If you don't get a stack overflow error / recursion error)
#
# Note: If you're using Python/PyPy, you might see a *slowdown* compared to above... 
# Don't be alarmed, you will solve all this in part 4.

Q3a:
    tiny:
      - buildNgramIndex:      [0.06s]
      - computeIntersections: [0.03s]
    small:
      - buildNgramIndex:      [1.20s]
      - computeIntersections: [0.56s]
    medium:
      - buildNgramIndex:      [1.26s]
      - computeIntersections: [0.57s]


# Which of the BSTs appearing in the program usually become unbalanced?
# ('fileNgrams', 'ngramIndex', or 'intersections')

Q3b:
    [fileNgrams, each file has different ngrams, and ngramIndex, each ngram has different files]

# And here is an OPTIONAL EXTRA QUESTION for those who are interested:
# Is there a simple way to stop these trees becoming unbalanced?
# (i.e., without using a self-balancing data structure)

Q3c:
    [...]


#########################################################################################
[PART 4] Implementing an AVL tree
#########################################################################################

# How long time does it take to (1) build the n-gram index, and (2) compute the intersections, 
# for the 'small', 'medium' and 'big' directories? 

Q4a:
    small:
      - buildNgramIndex:      [0.04s]
      - computeIntersections: [0.04s]
    medium:
      - buildNgramIndex:      [0.18s]
      - computeIntersections: [0.09s]
    big:
      - buildNgramIndex:      [2.75s]
      - computeIntersections: [0.75s]


# For the below questions, we denote by N the total number of 5-grams.
# We assume there is a (small) constant number of duplicate occurrences of 5-grams.

# What is the asymptotic complexity of 'buildNgramIndex'? 
# Justify briefly.

Q4b:
    Complexity:    [O(N * log(n))]
    Justification: [There are D documents that the buildNgramIndex method get access to, in our implementation we used a for loop which check every files,
                    this have a O(D) time complexity.
                    The implementation also assign each N-gram extracted from these files to ordered indexes and then assign each index to specific N-gram, this have a O(Log(n)) time complexity
                    In total the time complexity is O(N*log(n))]


# What is the asymptotic complexity of 'computeIntersections'? 
# Justify briefly.

Q4c:
    Complexity:    [O(N * log(n))]
    Justification: [Just because as the task mentioned we are looping through a small amount of intersecting paths so we can
                    neglect the nested loops and its time complexity. Instead, the outer for loop have n iteration and the "get"
                    function have time complexity of O(log(n)). In total the time complexity is O(n*log(n)) ]


# The 'huge' directory contains 4 times as many n-grams as the 'big'.
# Approximately how long time will it take to run the program on 'huge',
# given that you know how long time it took to run on 'big' (or 'medium')?
# Justify briefly.
#
# If you have the patience you can also run it to see how close your calculations were.

Q4d:
    Theoretical time to run 'huge': [27.6s]
    Justification: [Since the complexity of running buildIndex and computeInteersections is of 2nlogn cmplexity,
    if n increases by a factor of 4, the total change factor of the complexity of buildIndex and computeIntersections is
    8*2nlogn, so 8 times bigger.]

    (Optional) Actual time to run 'huge': [17.62s]


# Compare your answer in Q4d, with your answer in Q1d.

Q4e: [It seems to have gotten a bit faster. A quadratic complexity growth is much faster than our newer algorithm which grows linearithmicly.]


# OPTIONAL EXTRA QUESTION:
# Instead of the previous assumption, we now allow an arbitrary total similarity score S. 
# What is the asymptotic complexity of the two functions in terms of both N and S (at the same time)?

Q4f:
    Complexity of 'buildNgramIndex': [...]
    Justification: [...]

    Complexity of 'computeIntersections': [...]
    Justification: [...]


###############################################################################
[APPENDIX] General information
###############################################################################

# Approximately how many hours did you spend on the assignment, per group member?

Hours:
    [6h]


# Are there any known bugs or limitations?

Bugs:
    [No]

Limitations:
    [No, or maybe we didn't encounter]


# Did you collaborate with any other students on this lab?
# 
#   If so, please write in what way you collaborated and with whom.
#   Also include any resources (including the web) that you may
#   have used in creating your design.

Collaborations: 
    [No]


# Describe any serious problems you encountered.

Problems: 
    [We have not encountered any serious problem while doing labs (no)]


# List any other comments here.
#
#   Feel free to provide any feedback on how much you learned
#   from doing the assignment, and whether you enjoyed it.

Comments: 
    [Yes]


